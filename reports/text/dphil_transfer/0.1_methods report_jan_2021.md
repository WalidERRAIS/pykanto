---
title: "Brief Methods Report" 
author: Nilo M. Recalde
supervisors: Professor Ben Sheldon and Dr. Ella Cole
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
link-citations: true
colorlinks: true
secnumdepth: 0
numbersections: true
toc: false
lof: false
strip-comments: true
documentclass: report
output:
   bookdown::pdf_document2:
     includes:
      in_header: preamble.tex
     latex_engine: xelatex
subparagraph: true
bibliography: "`r here::here('reports', 'text', 'library.bib')`"
csl: "`r here::here('reports', 'text', 'animal-behaviour.csl')`"
---
\tableofcontents
\listoffigures

\begin{figure}[hbtp]
\includegraphics[width=1\paperwidth, center]{figures/wytham-2020.png}
\end{figure}



# Methods{#methods}

## Study system{#study-system}

The study was carried out in Wytham Woods, Oxfordshire, UK (51°46′N, 1°20′W). Wytham woods is a semi-natural deciduous woodland, around 415 hectares in extension, which is surrounded by farmland. Here, a population of great tits is monitored as part of a long-term survey that began in 1947. The majority of great tits nests in nestboxes with known locations. Every year, fieldworkers record the identities of breeding males and females, the dates of clutch initiation and egg hatching, clutch size, and fledgling success under standardised protocols. A large proportion of birds in the population are fitted with a unique British Trust for Ornithology (BTO) metal leg ring either as nestlings or as adults. During the breeding season, from March to June, great tit pairs are socially monogamous and defend territories around their nestboxes [@Hinde1952].

## Song recording

Every nestbox in the study site is checked by fieldworkers at least once a week before and during egg-laying, which can last from one to 14 days [@Perrins1965]. When a nestbox was marked as having great tit activity, which usually coincided with the laying of the first eggs, I placed an autonomous sound recorder in the vicinity of the nestbox—either in the same tree or in a suitable neighbouring tree. I recorded birds in this manner from early April until mid-May, leaving each recorder in the same location for three consecutive days before moving it to a different nestbox. I relocated ten recorders every day throughout the duration of the recording period.

I used 30 AudioMoth recorders  [@Hill2019b], which were housed in waterproof, custom-built enclosures. Recording began approximately one hour before sunrise (~ 05:36 – 04:00 UTC) and consisted of seven 59-minute-long (one hour minus one-minute pauses) recordings with a sample rate of 48 kHz.

&nbsp;

\begin{figure}[h] 
  \includegraphics[width=1\textwidth, center]{figures/sample_segmented.png}  
  \caption[Two different song types sung by a male]{Two different song types sung by a male. Each colour represents a different note; a combination of two different notes constitutes a phrase, and each line is a complete song.}  
  \label{fig:sample-segmented} 
\end{figure}

## Definitions

There is not a consistent set of terms used to refer to the different levels at which the acoustic output of a bird can be described. For clarity, these are the definitions that I use throughout this work (see also \autoref{fig:sample-segmented}):

| Term          | Definition                                                   |
| :------------ | :----------------------------------------------------------- |
| **Note**      | A single uninterrupted vocalisation; the smallest unit of analysis |
| **Phrase**    | The smallest set of different notes that are repeated stereotypically |
| **Song**      | One or more repeated phrases preceded and followed by silences of a duration exceeding that of the longest silence between each note in a phrase |
| **Song bout** | A set of one or more songs that are preceded and followed by silences longer than 10 seconds. I will change this to a probabilistic definition based on the distribution of silence durations; typically a more or less arbitrary threshold is used but I’d rather have a better-grounded definition |

## Audio analysis

### Pre-processing and segmentation

#### Song segmentation

I inspected spectrograms for each raw recording aided by AviaNZ, an open-source Python program written by Marsland and colleagues [@Marsland2019].  I selected songs based on a simple criterion: that its notes were clearly distinct from background noise and other bird vocalisations. I chose entire songs where it was possible; where it was not, I selected the longest contiguous segment available.

I included songs produced from approximately one hour before sunrise to four hours after sunrise for each bird and day. If a 59-min recording solely contained rain or wind sounds I also included the following 59-min recording to maximise the chances of recording a bird.

#### Assigning song bouts to individuals

As a consequence of the automated nature of the recording process, there is a small chance that some of the songs recorded in the immediate vicinity of a given nest box do not belong to the focal bird. To minimise the chance of false positives, I discarded recordings with more than one vocalising bird if one was not distinctly louder than the rest. I also discarded all songs with a maximum amplitude below $-16$ dB, calculated as $20\log_{10}(\frac{A}{A_0})$, with $A= 5000$ and $A_0=32767$ (the maximum value for 16-bit digital audio). This threshold derives from the observation that, in those cases where there are simultaneous recordings of several immediate neighbours, an amplitude cutoff greater than 4000 always separates a focal bird from its nearest neighbours. Note that these are not calibrated values and are, therefore, relative to the recording equipment and settings I used—as well as other factors like sound directionality and vegetation cover.

\begin{figure}[h]
  \includegraphics[width=1\textwidth, center]{figures/sample_segmentation.png}
  \caption[An illustration of the note segmentation process]{An illustration of the automated note segmentation process based on dynamic thresholds.}
  \label{fig:sample-segmentation}
\end{figure}

#### Note segmentation

I segmented the resulting song selections into their constituent notes using a custom dynamic threshold algorithm,  briefly described below:

1. The algorithm first reduces the reverberation present in the data by correcting the amplitude of points after local maxima within each frequency bin. This step is partly a Python port of JavaScript code by Robert Lachlan, as implemented in Luscinia (https://github.com/rflachlan/Luscinia).

2. The next step applies a multidimensional Gaussian filter, with a kernel of $\sigma = 1 $  truncated at $4 \sigma$, as well as a median filter, to reduce noise.

3. Finally, the algorithm finds minima in the spectral envelope of a spectrogram, which are considered silences; if the length of the signal between these minima exceeds a maximum note duration, a new local minimum is defined that divides the signal into two shorter segments. This is repeated until multiple notes are defined or there are no local minima below a maximum amplitude threshold. Then, segments below a minimum note duration threshold are discarded. This last step is partly based on a dynamic threshold algorithm implemented by @Sainburg2019b.

The minimum and maximum note length thresholds were determined by segmenting a small subset of songs (n = 30) with Chipper, an open-source, Python-based software developed by @Searfoss2020. \autoref{fig:sample-segmentation} shows an example of segmentation in a simple case.

\begin{figure}[H]
  \includegraphics[width=1\textwidth, center]{figures/MP68_sample.png}
  \caption[A small sample of notes from a male after segmentation and thresholding]{A small sample of notes from a male after preprocessing and segmentation.}
  \label{fig:sample-notes}
\end{figure}

#### Spectrograms

I created spectrograms for each individual note in the dataset from its normalised and band-passed waveform. I then log-scaled each spectrogram and clipped all values within the fifth lowest percentile, to remove background noise of low amplitude. I then zero-padded each spectrogram with length below the longest note and built a dataset containing the metadata for each note and its spectrogram. See \autoref{fig:sample-notes} for an example containing the processed spectrograms of a few notes in the dataset.

\begin{figure}[H]
  \includegraphics[width=1.06\textwidth, center]{figures/UMAP_scatter.png}
  \caption[UMAP embedding of the complete note dataset]{Embedding of the complete dataset using UMAP (Uniform Manifold Approximation and Projection, n = 182.616 notes). Colours represent distance in metres from a randomly selected point in the population.}
  \label{fig:umap}
\end{figure}

### Dimensionality reduction and clustering

#### At the population level

I prepared a $N × d$-dimensional array, with $N =$ total number of notes in the dataset and $d = 64 × 132$, or the length of a flattened two-dimensional spectrogram array. I then projected the first array onto a low-dimensional embedding found using UMAP [Uniform Manifold Approximation and Projection, @McInnes2018] and PHATE  [Potential of Heat-diffusion for Affinity-based Trajectory Embedding, @Moon2019], two non-linear manifold learning and dimensionality reduction algorithms. Full details of the implementation and parameters can be found in the corresponding [code module](#code-availability); see \autoref{fig:umap} for an example of a projection into two dimensions.

#### For individual birds

In a similar way, I used UMAP to project every note sung by each bird onto a lower-dimensional space. Specifically, I created a two-dimensional projection for visualisation and a 5-dimensional projection for clustering. I then used the latter to infer the note types sung by each bird, by finding areas occupied more densely within the acoustic space using HDBSCAN [Hierarchical Density-Based Spatial Clustering of Applications with Noise, @McInnes2017, see \autoref{fig:sample-repertoire}]. After labelling, I checked every bird using a custom-built interactive tool to ensure that there were no anomalous or mislabelled clusters (for example, noise-only clusters or notes from other birds in the background),  and manually corrected labels when necessary.

==INTERACTIVE TOOL PLOT==

\begin{figure}[H]
  \includegraphics[width=1.1\textwidth, center]{figures/SW5_repertoire.png}
  \caption[The entire repertoire of a great tit male]{The entire repertoire of a single great tit male, with examples of each note type.}
  \label{fig:sample-repertoire}
\end{figure}

### Inferring song types

Once individual notes were labelled, I used a custom algorithm to infer the distinct song types (i.e., stable combinations of notes) sung by each bird. The algorithm can be found here: https://github.com/nilomr/0.0_great-tit-song/blob/master/src/greti/sequencing/seqfinder.py. Very briefly, it finds n-grams that are repeated within songs, with element order determined by its first occurrence, and with multiple steps that ensure robustness against noise.

\begin{figure}[H]
  \includegraphics[width=1.15\textwidth, center]{figures/SW5_2020.png}
  \caption[An illustration of the process used to classify notes and transitions]{An illustration of the process used to classify notes and transitions for a random male in the dataset. \textbf{A}: Note clusters are defined using HDBSCAN, a density-based algorithm, on a 10-dimensional UMAP projection. \textbf{B}: a representation of the transitions between notes, where each line joins two notes sung consecutively. \textbf{C}: a directed graph with weights = first-order Markov transition probabilities between notes. \textbf{D}: Examples extracted at random from each cluster defined in A.}
  \label{fig:transitions}
\end{figure}

### Measuring acoustic similarity

I randomly sub-sampled the song type dataset to obtain a balanced set of songs per song type and bird, resulting in 3967 songs, or ~10 examples per song and bird. I then extracted the dominant frequency of each song using warbleR [@Araya-Salas2017], measured in 20 equidistant points to allow direct comparison, and calculated the dynamic time warping distances between each pair of frequency time-series with a fast implementation of the algorithm in dtwclust [@Sarda-Espinosa2019], using a window size of 10 (that is, 10% of the average signal length). 

### Song-type sharing

To automatically infer song type sharing, I clustered the songs into types by building a tree via hierarchical (UPGMA) clustering [fastcluster; @Mullner2013], and then extracted clusters by dynamically cutting the tree with the dynamicTreeCut package [@Langfelder2008]. (Add cluster validity measures, robustness checks, etc). I then calculated the Jaccard similarity index $J$ for each pair $A, B$ of birds, that is, $J(A, B) = \frac{|A \bigcap B|}{ |A\bigcup B|}$, which in this case is the size of the intersection of two repertoires divided by the size of their union.

### Models of sampling success

I modelled the number of recorded songs in a zero-inflated negative binomial generalized additive model, with the lag from the onset of egg-laying to the date of recording, the april lay date, and their interaction as population-level effects. I fit these models with the brms package [@Burkner2017a] and compared them using leave-one-out cross validation  [@Vehtari2017a]; the model presented predicts the data most accurately among those compared.

## Code availability:

The code necessary to reproduce all the analyses and figures in this report, along with more details about each method employed here, is available as an installable Python package from [github.com/nilomr/0.0_great-tit-song](https://github.com/nilomr/0.0_great-tit-song). Note: this repository is not yet public; contact me for access.



# Literature cited

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}

