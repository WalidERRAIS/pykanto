# %%
# %load_ext autoreload
# %autoreload 2

import pickle
from collections import ChainMap
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from joblib import Parallel, delayed
from src.avgn.utils.paths import ensure_dir, most_recent_subdirectory
from src.greti.clustering.indvs import cluster_individual, project_individual
from src.greti.read.paths import DATA_DIR
from tqdm.autonotebook import tqdm

# %%

# ### get data

DATASET_ID = "GRETI_HQ_2020_segmented"
YEAR = "2020"
n_jobs = -1

note_df_dir = (
    most_recent_subdirectory(DATA_DIR / "syllable_dfs" / DATASET_ID, only_dirs=True) / "{}.pickle".format(DATASET_ID)
)  # This gets the last dataframe generated by the previous script (in /prepare-data)
syllable_df = pd.read_pickle(note_df_dir)

# %%

indvs = [
    ind
    for ind in syllable_df.indv.unique()#[10:12]  #!!!! Remove subsetting !!!!
    if len(syllable_df[syllable_df.indv == ind])
    > 80  # This threshold is based on the need to have clusters >1 member
]

len(indvs)

# %%
# Number of syllables per nest - will need this later
syllable_n = pd.Series(
    [len(syllable_df[syllable_df.indv == ind]) for ind in syllable_df.indv.unique()]
)

# %%
# Projections (for clustering and visualisation)
# + Note sequence information

indv_dfs = {}

with Parallel(n_jobs=n_jobs, verbose=2) as parallel:
    indv_dfs = parallel(
        delayed(project_individual)(syllable_df, indv_dfs, indv, syllable_n)
        for indv in tqdm(indvs, desc="projecting individuals", leave=False)
    ) 

indv_dfs = dict(ChainMap(*indv_dfs))

# %%
# Cluster using HDBSCAN

with Parallel(n_jobs=n_jobs, verbose=2) as parallel:
    indv_dfs_labelled = parallel(
        delayed(cluster_individual)(indv_dfs, indv)
        for indv in tqdm(indv_dfs.keys(), desc="clustering individuals", leave=False)
    ) 

indv_dfs_labelled = dict(ChainMap(*indv_dfs_labelled))

# %%

# Prepare output directory
out_dir = DATA_DIR / "indv_dfs" / DATASET_ID
ensure_dir(out_dir)

# Save all dataframes in a single object
pickle.dump(indv_dfs_labelled, open(out_dir / (f"{DATASET_ID}_labelled.pickle"), "wb"))

# Save dataframe for each individual
out_dir = DATA_DIR / "indv_dfs" / DATASET_ID
ensure_dir(out_dir)
for indv in tqdm(indv_dfs_labelled.keys()):
    indv_dfs_labelled[indv].to_pickle(out_dir / (indv + "_labelled.pickle"))

