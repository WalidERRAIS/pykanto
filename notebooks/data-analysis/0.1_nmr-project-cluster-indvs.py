# %%
# from IPython import get_ipython

# get_ipython().run_line_magic("env", "CUDA_DEVICE_ORDER=PCI_BUS_ID")
# get_ipython().run_line_magic("env", "CUDA_VISIBLE_DEVICES=2")
%load_ext autoreload
%autoreload 2
# get_ipython().run_line_magic("matplotlib", "inline")

from os import error, wait
import pickle
from datetime import datetime

import hdbscan
from matplotlib import colors
import matplotlib.pyplot as plt
import numpy as np
from numpy.lib.shape_base import column_stack
import pandas as pd
import phate
import seaborn as sns
import umap
from joblib import Parallel, delayed
from scipy.spatial import distance
from sklearn.decomposition import PCA
from src.avgn.signalprocessing.create_spectrogram_dataset import (
    create_syllable_df, flatten_spectrograms, log_resize_spec)
from src.avgn.utils.general import save_fig
from src.avgn.utils.paths import ensure_dir, most_recent_subdirectory
from src.avgn.visualization.network_graph import plot_network_graph
from src.avgn.visualization.projections import (scatter_projections,
                                                scatter_spec)
from src.avgn.visualization.quickplots import (draw_projection_plots,
                                               quad_plot_syllables)
from src.avgn.visualization.spectrogram import draw_spec_set
from src.greti.read.paths import DATA_DIR, FIGURE_DIR, RESOURCES_DIR
from tqdm.autonotebook import tqdm

# from sklearn.cluster import MiniBatchKMeans


# import importlib
# importlib.reload(src)

# %%

# ### get data

DATASET_ID = "GRETI_HQ_2020_segmented"
YEAR = "2020"

note_df_dir = (
    most_recent_subdirectory(DATA_DIR / "syllable_dfs" / DATASET_ID, only_dirs=True) / "{}.pickle".format(DATASET_ID)
)  # This gets the last dataframe generated by the previous script (in /prepare-data)
syllable_df = pd.read_pickle(note_df_dir)

# %%

indvs = [
    ind
    for ind in syllable_df.indv.unique()[13:15]  #!!!! Remove subsetting !!!!
    if len(syllable_df[syllable_df.indv == ind])
    > 80  # This threshold is based on the need to have clusters >1 member
]

len(indvs)

# %%
# Number of syllables per nest - will need this later
syllable_n = pd.Series(
    [len(syllable_df[syllable_df.indv == ind]) for ind in syllable_df.indv.unique()]
)

# %%
# Colours

facecolour = "#f2f1f0"
colours = [
    "#66c2a5",
    "#fc8d62",
    "#8da0cb",
    "#e78ac3",
    "#a6d854",
    "#ffd92f",
    "#e5c494",
    "#b3b3b3",
    "#fc6c62",
    "#7c7cc4",
    "#57b6bd",
    "#e0b255",
]

pal = sns.set_palette(sns.color_palette(colours))

# %%
# # Projections (for clustering and visualisation)
# + Note sequence information

indv_dfs = {}

for indvi, indv in enumerate(tqdm(indvs)):

    indv_dfs[indv] = syllable_df[syllable_df.indv == indv]
    indv_dfs[indv] = indv_dfs[indv].sort_values(by=["key", "start_time"])
    print(indv, len(indv_dfs[indv]))
    specs = [i for i in indv_dfs[indv].spectrogram.values]

    with Parallel(n_jobs=-2, verbose=2) as parallel:
        specs = parallel(
            delayed(log_resize_spec)(spec, scaling_factor=8)
            for spec in tqdm(specs, desc="scaling spectrograms", leave=False)
        )

    # Add note sequences to dataframe for later use
    indv_dfs[indv]["syllables_sequence_id"] = None
    indv_dfs[indv]["syllables_sequence_pos"] = None
    for ki, key in enumerate(indv_dfs[indv].key.unique()):
        indv_dfs[indv].loc[indv_dfs[indv].key == key, "syllables_sequence_id"] = ki
        indv_dfs[indv].loc[
            indv_dfs[indv].key == key, "syllables_sequence_pos"
        ] = np.arange(np.sum(indv_dfs[indv].key == key))

    specs_flattened = flatten_spectrograms(specs)

    # # PHATE
    # phate_operator = phate.PHATE(n_jobs=-1, knn=5, decay=None, t=110, gamma=0)
    # z = list(phate_operator.fit_transform(specs_flattened))
    # indv_dfs[indv]["phate"] = z

    # # PHATE_cluster
    # phate_operator = phate.PHATE(n_jobs=-1, knn=5, decay=30, n_components=5)
    # z = list(phate_operator.fit_transform(specs_flattened))
    # indv_dfs[indv]["phate_cluster"] = z

    # pca = PCA(n_components=2)
    # indv_dfs[indv]["pca_viz"] = list(pca.fit_transform(specs_flattened))

    # pca2 = PCA(n_components=5)
    # indv_dfs[indv]["pca_cluster"] = list(pca2.fit_transform(specs_flattened))

    # # umap_cluster 
    # fit = umap.UMAP(n_neighbors=20, min_dist=0.05, n_components=10, verbose=True)
    # z = list(fit.fit_transform(specs_flattened))
    # indv_dfs[indv]["umap_cluster"] = z

    # Set min distance (for visualisation only) depending on # syllables
    # min_dist = (
    #     ((len(specs_flattened) - min(syllable_n)) * (0.4 - 0.1))
    #     / (max(syllable_n) - min(syllable_n))
    # ) + 0.1

    # umap_viz
    #n_neighbors=60, min_dist=min_dist, n_components=2, verbose=True
    fit = umap.UMAP(n_components=2, min_dist=0.1)
    z = list(fit.fit_transform(specs_flattened))
    indv_dfs[indv]["umap_viz"] = z


# %%
# Cluster using HDBSCAN

for indv in tqdm(indv_dfs.keys()):
    z = list(indv_dfs[indv]["umap_viz"].values)
    min_cluster_size = int(len(z) * 0.04) # smallest cluster size allowed
    if min_cluster_size < 2:
        min_cluster_size = 2
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size,  
        min_samples=10,  # larger values = more conservative clustering
        cluster_selection_method="eom",
    )
    clusterer.fit(z)
    indv_dfs[indv]["hdbscan_labels"] = clusterer.labels_

    # # Plot
    # n_colours = len(indv_dfs[indv]["hdbscan_labels"].unique())
    # color_palette = sns.color_palette("deep", n_colours)
    # cluster_colors = [
    #     color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_
    # ]
    # cluster_member_colors = [
    #     sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)
    # ]

    # x = np.array(list(indv_dfs[indv]["umap_viz"].values))[:, 0]
    # y = np.array(list(indv_dfs[indv]["umap_viz"].values))[:, 1]
    # plt.scatter(x, y, s=10, linewidth=0, c=cluster_member_colors, alpha=0.3)
    # plt.show()

    # clusterer.condensed_tree_.plot(
    #     select_clusters=True, selection_palette=sns.color_palette("deep", 14)
    # )

    # plt.show()
    
    # # Plot outliers
    # sns.distplot(clusterer.outlier_scores_[np.isfinite(clusterer.outlier_scores_)], rug=True)
    # plt.show()
    # threshold = pd.Series(clusterer.outlier_scores_).quantile(0.99)
    # outliers = np.where(clusterer.outlier_scores_ > threshold)[0]
    # plt.scatter(x,y, s=10, linewidth=0, c='gray', alpha=0.25)
    # plt.scatter(x[outliers], y[outliers], s=10, linewidth=0, c='red', alpha=0.5)
    # plt.show()

    # Count labels
    print(indv + ":" + str(len(indv_dfs[indv]["hdbscan_labels"].unique())))

# %%

# Save dataframe for each individual
out_dir = DATA_DIR / "indv_dfs" / DATASET_ID
ensure_dir(out_dir)


for indv in tqdm(indv_dfs.keys()):
    indv_dfs[indv].to_pickle(out_dir / (indv + ".pickle"))



# %%
# ### plot each individual's repertoire


for indv in tqdm(indv_dfs.keys()):
    labs = indv_dfs[indv]["hdbscan_labels"].values
    proj = np.array(list(indv_dfs[indv]["umap_viz"].values))
    specs = indv_dfs[indv].spectrogram.values

    scatter_spec(
        proj,
        specs=indv_dfs[indv].spectrogram.values,
        column_size=8,
        # x_range = [-5.5,7],
        # y_range = [-10,10],
        pal_color="hls",
        color_points=False,
        enlarge_points=20,
        range_pad=0.1,
        figsize=(10, 10),
        scatter_kwargs={
            "labels": labs,
            "alpha": 0.60,
            "s": 5,
            "color_palette": pal,
            "show_legend": False,
        },
        matshow_kwargs={"cmap": plt.cm.Greys},
        line_kwargs={"lw": 1, "ls": "solid", "alpha": 0.11},
        draw_lines=True,
        border_line_width=0,
        facecolour=facecolour,
    )

    fig_out = (
        FIGURE_DIR
        / YEAR
        / "ind_repertoires"
        / (
            "{}_repertoire_".format(indv)
            + str(datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))
            + ".png"
        )
    )
    ensure_dir(fig_out)
    plt.savefig(
        fig_out, dpi=300, bbox_inches="tight", pad_inches=0.3, transparent=False,
    )
    plt.show()
    plt.close()


# %%

# Plot: scatter, transitions, examples, per nestbox
# Saves figures to FIGURE_DIR / year / "ind_repertoires" / (indv + ".png")


quad_plot_syllables(indv_dfs, YEAR, "umap_viz", palette=pal, facecolour=facecolour)

# %%
import string
from datetime import datetime

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from src.avgn.utils.paths import ensure_dir, most_recent_subdirectory
from src.avgn.visualization.network_graph import plot_network_graph
from src.avgn.visualization.projections import (
    draw_projection_transitions,
    plot_label_cluster_transitions,
    scatter_projections,
    scatter_spec,
)
from src.avgn.visualization.spectrogram import draw_spec_set, plot_example_specs
from src.greti.read.paths import DATA_DIR, FIGURE_DIR
from tqdm.autonotebook import tqdm
from matplotlib.lines import Line2D

#%%
viz_proj="umap_viz"
pal="tab20"
#indv = 'MP42'

for indv in tqdm(indv_dfs.keys()):

    f = plt.figure(figsize=(10, 10))
    gs = f.add_gridspec(ncols=3, nrows=2, width_ratios=[1, 1, 1], height_ratios = [0.5, 1], hspace=0.1, wspace=0.2)

    axes = [f.add_subplot(gs[i]) for i in range(3)]
    axes = axes + [f.add_subplot(gs[1, :])]

    f.suptitle("Syllable clusters and transitions for {}".format(indv), fontsize=16,)

    hdbscan_labs = indv_dfs[indv]["hdbscan_labels_fixed"]
    labs = hdbscan_labs.values
    unique_labs = hdbscan_labs.unique()
    nlabs = len(unique_labs)


    proj = np.array(list(indv_dfs[indv][viz_proj].values))[:, 0:2]
    sequence_ids = np.array(indv_dfs[indv]["syllables_sequence_id"])
    specs = np.invert(indv_dfs[indv].spectrogram.values)
    specs = np.where(specs == 255, 242, specs)  # grey

    palette = sns.color_palette(pal, n_colors=len(np.unique(labs)))

    # Projection scatterplot, labeled by cluster
    scatter_projections(
        projection=proj,
        labels=labs,
        color_palette=palette,
        alpha=0.60,
        s=2,
        facecolour=facecolour,
        show_legend=False,
        range_pad=0.1,
        ax=axes[0],
    )


    # Draw lines between consecutive syllables
    draw_projection_transitions(
        projections=proj,
        sequence_ids=indv_dfs[indv]["syllables_sequence_id"],
        sequence_pos=indv_dfs[indv]["syllables_sequence_pos"],
        cmap=plt.get_cmap("ocean"),
        facecolour=facecolour,
        linewidth=0.8,
        range_pad=0.05,
        alpha=0.05,
        ax=axes[1],
    )

    # Plot inferred directed network
    plot_network_graph(
        labs,
        proj,
        sequence_ids,
        color_palette=palette,
        min_cluster_samples=0,
        min_connections=0,
        facecolour=facecolour,
        ax=axes[2],
        edge_width=1,
        point_size=60
    )

    # Plot examples of each cluster
    plot_example_specs(
        specs=specs,
        labels=labs,
        clusters_to_viz=unique_labs[unique_labs >= 0],  # do not show 'noisy' points
        custom_pal=palette,
        cmap=plt.cm.bone,
        nex=10,
        line_width=5,
        ax=axes[3],
    )

    # color labels

    lab_dict = {lab: palette[i] for i, lab in enumerate(np.unique(labs))}

    lab_dict[-1] = (
        0.83137254902,
        0.83137254902,
        0.83137254902
        )  # colour noisy data grey

    legend_elements = [
        Line2D([0], [0], marker="o", color=value, label=key)
        for key, value in lab_dict.items()
    ]

    axes[3].legend(handles=legend_elements, bbox_to_anchor=(1.04, 0.65))

    # labels = string.ascii_uppercase[0 : len(axes)]

    # for ax, labels in zip(axes, labels):
    #     bbox = ax.get_tightbbox(f.canvas.get_renderer())
    #     f.text(
    #         0.03,
    #         0.97,
    #         labels,
    #         fontsize=25,
    #         fontweight="bold",
    #         va="top",
    #         ha="left",
    #         transform=ax.transAxes,
    #     )

    plt.show()

# %%

# Interactive test

import plotly.express as px
df = px.data.iris()
fig = px.scatter(df, x="sepal_width", y="sepal_length", color="species")
fig.show()

scatter_projections(
    projection=proj,
    labels=labs,
    color_palette=palette,
    alpha=0.60,
    s=2,
    facecolour=facecolour,
    show_legend=False,
    range_pad=0.1,
    ax=axes[0],
)

#%%
# prepare data

def prepare_interactive_data(indv):

    global new_df, colour, palette

    labs = indv_dfs[indv]["hdbscan_labels"].values
    palette = sns.color_palette(pal, n_colors=len(np.unique(labs)))
    lab_dict = {lab: palette[i] for i, lab in enumerate(np.unique(labs))}
    lab_dict[-1] = (
        0.83137254902,
        0.83137254902,
        0.83137254902
        )

    x = np.array(list(indv_dfs[indv]["umap_viz"].values))[:, 0]
    y = np.array(list(indv_dfs[indv]["umap_viz"].values))[:, 1]
    # z = np.array(list(indv_dfs[indv]["umap_viz"].values))[:, 2]

    #colours = np.array([lab_dict[i] for i in labs])
    colour  = {f'{lab}' : f'rgb{tuple((np.array(color)*255).astype(np.uint8))}' for lab, color in lab_dict.items()}


    df = pd.DataFrame(data = np.column_stack((x.astype(np.object), y.astype(np.object), labs)), columns = ['x', 'y', 'labs'])
    df["labs"] = df["labs"].map(str)

    new_df = df

#%%
# Plot data

import plotly.express as px
import plotly.graph_objs as go
import plotly.offline as py

import pandas as pd
import numpy as np
from ipywidgets import interactive, HBox, VBox, widgets


def interactive_scatter():

    global fig, new_df

    newpalette = sns.color_palette(pal, n_colors=len(np.unique(new_df.labs)))

    newlab_dict = {lab: newpalette[i] for i, lab in enumerate(np.unique(new_df.labs)) if newpalette[i] not in palette}
    newlab_dict[-1] = (
        0.83137254902,
        0.83137254902,
        0.83137254902
        )
    newcolour  = {f'{lab}' : f'rgb{tuple((np.array(colour)*255).astype(np.uint8))}' for lab, colour in newlab_dict.items()}

    newentries = {lab : code for lab, code in newcolour.items() if lab not in colour.keys()}
    colour.update(newentries)

    fig = px.scatter(new_df, x="x", y="y", color = "labs", color_discrete_map= colour)

    fig.update_xaxes(showgrid=False, zeroline=False, visible=False, showticklabels=False)
    fig.update_yaxes(showgrid=False, zeroline=False, visible=False, showticklabels=False)

    fig.update_layout(
        autosize=False,
        width=600,
        height=600,
        legend=dict(
        orientation="v"),
        legend_title_text='Label',
        title={
        'text': f"{indv}",
        'y':0.935,
        'x':0.5,
        'xanchor': 'center',
        'yanchor': 'top'},
        xaxis_range=(new_df.x.min() - 1, new_df.x.max() + 1),
        yaxis_range=(new_df.y.min() - 1, new_df.y.max() + 1),
        plot_bgcolor=facecolour,

    )

    fig  = go.FigureWidget(fig)

    return fig

# %%
# Cross-reference indexes

def change_label(label_to_assign = "666"):

    global fig, new_df

    if not isinstance(label_to_assign, str):
        raise Exception("Label is not a string")

    selection = []
    for f in fig.data:
        for name, name_new_df in new_df.groupby('labs'):
            if f.name == name:
                selection = selection + [name_new_df.iloc[i].name for i in f.selectedpoints]

    
    for i in selection:
        new_df.loc[i, 'labs'] = label_to_assign


    fig = interactive_scatter()
    fig.update_layout(
    title={
        'text': f"{indv}: Changed {len(selection)} points to label '{label_to_assign}'",
        'y':0.935,
        'x':0.5,
        'xanchor': 'center',
        'yanchor': 'top'})

    return fig

    # #TODO: THIS DOESN'T WORK
    # fig2  = go.FigureWidget(fig2)
    # for f, f2 in itertools.zip_longest(fig.data, fig2.data):
    #     if f2.name == f.name:
    #         f.legendgroup = f2.legendgroup
    #         f.y = f2.y
    #         f.x = f2.x
    #         f.fillcolor = f2.fillcolor
    #         f.fill = f2.fill
    #         f.marker = f2.marker
    #         f.selectedpoints = []
    #     elif f is None:
    #         f2.selectedpoints = []
    #         fig.add_trace(f2)
    #     elif f.name not in np.unique(new_df.labs):
    #         for element in f:
    #             f[element = []

# %%
#! CAREFUL
i = -1
already_checked = []
#! CAREFUL


# %%
i += 1

if i >= len(indvs):
    raise Exception("End of list")
else:
    print(len(indvs))
    indv = indvs[i]
    already_checked.append(indv)




prepare_interactive_data(indv)
interactive_scatter()


# %%

indv_dfs_tmp = indv_dfs

if len(indv_dfs_tmp[indv]["hdbscan_labels"]) == len(new_df):
    print('lol')
    indv_dfs_tmp[indv]["hdbscan_labels_fixed"] = [int(i) for i in new_df.labs]

progress_out = DATA_DIR / 'resources' / DATASET_ID / 'label_fix_progress' / f'progress_{str(datetime.now().strftime("%Y-%m-%d_%H-%M"))}.txt'
ensure_dir(progress_out)

with open(progress_out, "w") as output:
    output.write(str(already_checked))
# %%
